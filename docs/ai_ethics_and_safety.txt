【社会伦理】当机器拥有智能（Intelligence）：AI 安全与对齐问题

    随着 GPT-4 等模型的发布，人工智能（Intelligence）的能力边界被不断拓展。这引发了一个严肃的问题：我们如何确保超级智能的目标与人类的利益相一致？这在学术界被称为“对齐问题”（Alignment Problem）。如果不加控制，一个强大的优化算法可能会为了达成目标而采取极端的手段。
    
    目前的解决方案包括人类反馈强化学习（Reinforcement Learning from Human Feedback, RLHF），即让人类标注员对模型的输出进行打分，引导模型学习（Learning）符合人类价值观的行为。
    
    此外，随着 AI 被广泛集成到 Python 代码库和云计算（Cloud）服务中，系统的安全性也变得至关重要。防止模型被注入恶意指令或输出有害信息，是当前 AI 安全研究的重点。机器人（Robotics）三定律在今天显得尤为重要和紧迫。